{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from tensorflow.keras import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_csv_files(directory,activities_of_interest = None):\n",
    "    csv_files = []\n",
    "    for current_folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                if activities_of_interest is not None:\n",
    "                    for activity in activities_of_interest:\n",
    "                        if activity in file:\n",
    "                            full_path = os.path.join(current_folder, file)\n",
    "                            csv_files.append(full_path)\n",
    "                            break\n",
    "                else:\n",
    "                    full_path = os.path.join(current_folder, file)\n",
    "                    csv_files.append(full_path)\n",
    "                    break\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "falls_files = search_csv_files(directory='output', activities_of_interest=['Activity13', 'Activity14', 'Activity15'])\n",
    "adl_files = search_csv_files(directory='output', activities_of_interest=['Activity1', 'Activity2', 'Activity3', 'Activity4', 'Activity5', 'Activity6', 'Activity7', 'Activity8', 'Activity9', 'Activity10', 'Activity11', 'Activity12'])\n",
    "\n",
    "len(falls_files), len(adl_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_pad(file, use_gyro=False, max_len=300):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Define the columns to use\n",
    "    if use_gyro:\n",
    "        cols = ['Accelerometer: x-axis (g)', 'Accelerometer: y-axis (g)', 'Accelerometer: z-axis (g)',\n",
    "                'Gyroscope: x-axis (rad/s)', 'Gyroscope: y-axis (rad/s)', 'Gyroscope: z-axis (rad/s)']\n",
    "    else:\n",
    "        cols = ['Accelerometer: x-axis (g)', 'Accelerometer: y-axis (g)', 'Accelerometer: z-axis (g)']\n",
    "    \n",
    "    # Clean and convert data for each column\n",
    "    for col in cols:\n",
    "        if df[col].dtype == 'object':  # If column has string values\n",
    "            # Replace incorrect number format (assuming European format with '.' as thousand separator)\n",
    "            df[col] = df[col].astype(str).str.replace('.', '', n=1)  # Remove first period (thousand separator)\n",
    "            df[col] = pd.to_numeric(df[col].str.replace(',', '.'), errors='coerce')  # Replace comma with period for decimal\n",
    "\n",
    "    # Extract features and convert to numpy array\n",
    "    features = df[cols].to_numpy().astype('float32')\n",
    "    \n",
    "    # Pad or truncate as needed\n",
    "    if features.shape[0] > max_len:\n",
    "        features = features[:max_len, :]\n",
    "    elif features.shape[0] < max_len:\n",
    "        pad = np.zeros((max_len - features.shape[0], features.shape[1]), dtype='float32')\n",
    "        features = np.vstack((features, pad))\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(falls_files, adl_files, max_len=300):\n",
    "    X_acc, X_all, y = [], [], []\n",
    "\n",
    "    for file in falls_files:\n",
    "        X_acc.append(load_and_pad(file, use_gyro=False, max_len=max_len))\n",
    "        X_all.append(load_and_pad(file, use_gyro=True, max_len=max_len))\n",
    "        y.append(1)\n",
    "\n",
    "    for file in adl_files:\n",
    "        X_acc.append(load_and_pad(file, use_gyro=False, max_len=max_len))\n",
    "        X_all.append(load_and_pad(file, use_gyro=True, max_len=max_len))\n",
    "        y.append(0)\n",
    "\n",
    "    return np.array(X_acc), np.array(X_all), np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc, X_all, y = build_dataset(falls_files, adl_files)\n",
    "\n",
    "\n",
    "print(\"Accelerometer only:\", X_acc.shape)\n",
    "print(\"Accelerometer + Gyroscope:\", X_all.shape)\n",
    "print(\"Labels:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "USE_GYRO = False\n",
    "\n",
    "X = X_all if USE_GYRO else X_acc\n",
    "X_2d = X.reshape((X.shape[0], -1))\n",
    "\n",
    "# Remove samples with NaNs\n",
    "mask = ~np.isnan(X_2d).any(axis=1)\n",
    "X_clean = X_2d[mask]\n",
    "y_clean = y[mask]\n",
    "\n",
    "print(f\"Removed {len(y) - len(y_clean)} samples due to NaNs.\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_clean, y_clean)\n",
    "\n",
    "# Reshape back to 3D for CNN\n",
    "X_resampled = X_resampled.reshape((-1, 300, X.shape[2]))\n",
    "\n",
    "print(\"✅ SMOTE done:\")\n",
    "print(\"X shape:\", X_resampled.shape)\n",
    "print(\"Label counts:\", np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6508 - loss: 0.5976 - val_accuracy: 0.7962 - val_loss: 0.4916\n",
      "Epoch 2/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8327 - loss: 0.4352 - val_accuracy: 0.7962 - val_loss: 0.4644\n",
      "Epoch 3/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8217 - loss: 0.4297 - val_accuracy: 0.7962 - val_loss: 0.4808\n",
      "Epoch 4/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8328 - loss: 0.3986 - val_accuracy: 0.7962 - val_loss: 0.4827\n",
      "Epoch 5/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8540 - loss: 0.3474 - val_accuracy: 0.7915 - val_loss: 0.4736\n",
      "Epoch 6/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8463 - loss: 0.3422 - val_accuracy: 0.7915 - val_loss: 0.5449\n",
      "Epoch 7/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8265 - loss: 0.3863 - val_accuracy: 0.7915 - val_loss: 0.5321\n",
      "Epoch 8/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8283 - loss: 0.3318 - val_accuracy: 0.7820 - val_loss: 0.5039\n",
      "Epoch 9/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8439 - loss: 0.3377 - val_accuracy: 0.7962 - val_loss: 0.5249\n",
      "Epoch 10/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8612 - loss: 0.3217 - val_accuracy: 0.7867 - val_loss: 0.5208\n",
      "Epoch 11/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8656 - loss: 0.3061 - val_accuracy: 0.7915 - val_loss: 0.5812\n",
      "Epoch 12/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8467 - loss: 0.3430 - val_accuracy: 0.7962 - val_loss: 0.6087\n",
      "Epoch 13/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8666 - loss: 0.2951 - val_accuracy: 0.7773 - val_loss: 0.5817\n",
      "Epoch 14/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8674 - loss: 0.2975 - val_accuracy: 0.7820 - val_loss: 0.5888\n",
      "Epoch 15/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8541 - loss: 0.2963 - val_accuracy: 0.7725 - val_loss: 0.6376\n",
      "Epoch 16/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8633 - loss: 0.2829 - val_accuracy: 0.7915 - val_loss: 0.6077\n",
      "Epoch 17/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8788 - loss: 0.2736 - val_accuracy: 0.7773 - val_loss: 0.6251\n",
      "Epoch 18/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8628 - loss: 0.2736 - val_accuracy: 0.7867 - val_loss: 0.6768\n",
      "Epoch 19/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8570 - loss: 0.2960 - val_accuracy: 0.7773 - val_loss: 0.6588\n",
      "Epoch 20/20\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8717 - loss: 0.2669 - val_accuracy: 0.7867 - val_loss: 0.7047\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.6554\n",
      "\n",
      "Test Accuracy: 0.77\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x31c1951f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.67      0.74       132\n",
      "           1       0.72      0.87      0.79       132\n",
      "\n",
      "    accuracy                           0.77       264\n",
      "   macro avg       0.78      0.77      0.77       264\n",
      "weighted avg       0.78      0.77      0.77       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "USE_GYRO = False\n",
    "\n",
    "X = X_all if USE_GYRO else X_acc\n",
    "\n",
    "y = y  # already 0 (ADL) or 1 (Fall)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(300, X.shape[2])),\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Report\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
